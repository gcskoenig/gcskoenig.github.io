<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>If interpretability is the answer, what is the question? | Gunnar König</title>
<meta name="keywords" content="interpretability, taxonomy">
<meta name="description" content="I propose a taxonomy of the types of questions that IML methods can answer.">
<meta name="author" content="Gunnar König">
<link rel="canonical" href="https://gunnarkoenig.com/posts/2022/interpretability-levels/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css" integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js" integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://gunnarkoenig.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://gunnarkoenig.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://gunnarkoenig.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://gunnarkoenig.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://gunnarkoenig.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
onload="renderMathInElement(document.body, 
    {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '\\[', right: '\\]', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false}
              ]
          }
    );"></script>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-141772906-1', 'auto');
	
	ga('send', 'pageview');
}
</script><meta property="og:title" content="If interpretability is the answer, what is the question?" />
<meta property="og:description" content="I propose a taxonomy of the types of questions that IML methods can answer." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gunnarkoenig.com/posts/2022/interpretability-levels/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-16T00:30:03&#43;00:00" />
<meta property="article:modified_time" content="2022-04-16T00:30:03&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="If interpretability is the answer, what is the question?"/>
<meta name="twitter:description" content="I propose a taxonomy of the types of questions that IML methods can answer."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://gunnarkoenig.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "If interpretability is the answer, what is the question?",
      "item": "https://gunnarkoenig.com/posts/2022/interpretability-levels/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "If interpretability is the answer, what is the question?",
  "name": "If interpretability is the answer, what is the question?",
  "description": "I propose a taxonomy of the types of questions that IML methods can answer.",
  "keywords": [
    "interpretability", "taxonomy"
  ],
  "articleBody": "Introduction The goal of Interpretable Machine Learning (IML) is to provide human-intelligible descriptions of potentially complex ML models. However, IML descriptors themselves can be difficult to interpret: To reduce complexity, each method is limited to provide insight into a specific aspect of model and data. If the method’s focus is not clearly understood, IML descriptors can be misinterpreted.\nWe illustrate the problem on a simple example: Suppose a practitioner aims to learn about the relevance of in-store temperature for sales at a petrol station. Therefore she fits a model on a sales dataset and computes the Partial Dependence Plot (PDP) for the temperature (Figure 1).\nLooking at the PDP output, she decides to turn up the theromstat in order to increase the sales. Her conclusion is not valid: In reality, increasing in-store temperature has a small negative causal effect on the sales.1 The reason is that the chosen method, PDP, is designed to provide insight into the model’s mechanism, but in general does not allow insight into the effect of real-world actions.2\nIn this article, we aid practicitioners to chose suitable IML tools and to avoid misinterpretation of the IML descriptor. More specifically, we guide practitioners to\n carefully choose the aspect of model and data that shall be described to understand which IML descriptors are suitable for their task.  Therefore we present a taxonomy of the different aspects of model and data that may be of interest (here) and classify existing established model-agnostic IML methods within that taxonomy (here). 3\nWhich aspect of model and data shall be described? As follows, we distinguish nine different aspects of model and data. Therefore, we distinguish IML descriptions along two questions:\n Which object is described? Is it the model’s prediction $\\hat{Y}$, the underlying real-world object $Y$ or their relationship? On what level is the object described? I.e., are we interested in observing co-occurance or in understanding causal effects? And, if we are interested in causal effects: Are we interested in the effect of plugging in different values into the model, or into the effect of actions in the real-world?  As follows, we demonstrate that the objects/levels of understanding are distinct and discuss when each of the categories may be of interest.\nWhich object? Depending on the interpretation goal, we may either be interested in understanding\n the model’s prediction $\\hat{Y}$, the prediction target $Y$, the prediction loss $L$, or more generally the relationship between $\\hat{Y}$ and $Y$.  [TODO color code the bullets]\nDifferentiation: First of all, $Y$ and $\\hat{Y}$ are different objects: In general they take different values within the observational distribution (since in general $Y$ cannot be perfectly predicted from the covariates). Furthermore they take different causal roles and as such behave very differently in interventional environments. For example, changing the test result has a causal effect on the Covid prediction, but does not affect whether someone is infected with covid. Clearly, $Y$ and $\\hat{Y}$ are different objects than e.g. their loss $L$. For instance, the loss $L$ takes different values than $Y$ or $\\hat{Y}$ and also has a different causal role.\nSuitability: We have established that $Y$, $\\hat{Y}$ and $R$ are different objects; the question which of the objects is of interest in which scenario remains. The practitioner has to carefully assess which of those is of interest in any situation; we cannot give general recommendations. However, we can roughly categorize as follows. In scenarios where we trust the model and want to learn from it, e.g. in scientific inference, we typically aim to understand the underlying target $Y$. In these scenarios, the prediction is only a proxy for understanding $Y$. In scenarios where we are sceptical of the model and the goal is to debug it, we typically aim to understand the prediction $\\hat{Y}$. For instance, if the model relies on a protected attribute or a spuriously correlated variable, we would want to expose this, irrespective of whether that relationship is reflected in the underlying data generating process. Understanding their relationship is for example interesting in situations where we want to understand the sensitivity of the model’s performance to changes in the data generating process.\nOn what level? We have established that there are three distinct objects which all may be of interest depending on the situation. In this Section we explain that we can understand each of the objects on different levels. More specifically, we may either be interested in understanding\n How is the object associated with the covariates (statistical dependence)? [What does that mean exactly] How is the object affected by real-world action (i.e., interventions on light blue variables, $do(X_j=\\theta_j)$)? [What does that mean why is it interesting] How is the object affected by model-level interventions (i.e., interventions on purple variables, $do(\\overline{X}_j=\\theta_j)$)? ][What does that mean why is it interesting?]  [TODO color code the bullets]\nDifferentiation: For instance, suppose that we are interested in the model’s prediction. Data-level causation is different from model-level causation: For instance intervening on trust on the model level does not affect the prediction, but intervening on the data-level affects the prediction via the vaccination status. Furthermore causation is different from association: quarantine is associated with the prediction, but neither model-level intervention nor data-level intervention on quarantine affect the prediction.\nSubtitle: To illustrate our arguments, we make use of the example of a CoViD prediction model $\\hat{f}$. The model is trained on variables such as the vaccination status or quick-test results and shall predict the risk of someone having covid. For the example we assume knowledge of the corresponding structural causal model (SCM), which we can use to sample data or to predict the effect of interventions in the real-world. The combination of model’s mechanism and data generating process is illustrated below.\nSuitability: Describing objects with respect to the effects of data-level interventions is of interest if the description is meant to guide real-world interaction. For instance, recourse recommendations guide individuals towards reverting unfavorable decisions, or business analysists may be interested in understanding how to keep customers who are likely to churn. Estimating the effect of data-level interventions is in general difficult to obtain and not always necessary. For instance, to select the variables that are relevant to diagnose diseases understanding which variables are strongly associated with the disease is sufficient. It has also been argued that, in order to understand the model’s mechanism, we are interested in the effect of plugging in different values for each of the features to the model [cite janzing]. This allows for properties such as sensitivity, i.e. that nonzero effect implies nonzero coefficients.\nIt remains an open question when which of the levels is of interest. Typically either data-level causation or association. Data-level causation tells us how the the object of interest behaves if we act in the real-world. Association is helpful if we are only interested in co-occurance (which is e.g. required for prediction). The use of model-level causation is debated; some argue that this is exactly what we want if we are interested in in interpretability since we want to understand the model’s mechanism, others argue that interpretation of the model in unrealsitic regions is not helpful at all and that model-level interventions lead to extrapolation into unrealistic environments.\nClassification of Common IML Techniques Overview table with the 9 categories.\n    prediction target performance     association M-Plotconditional SHAP - conditional SAGECFIASV   causal (data-level) CRCSVs MCRHastie PDP Invariant prediction importance   causal (model-level) CXplainmarginal SHAP - PFImarginal SAGE    Relationships between categories In principle mutually exclusive. Demonstrate on examples in one figure (e.g. respective global relevance plot for each type in a 3x3 grid), proof in appendix.\napproach 1: assumptions and their consequences\n statistical independence of covariates causal independence of covariates observational identifiability of causal effect? bayes optimal predictor perfect predictor  approach 2: for each of the 9 classes, list which other classes imply the class and under which assumption they do so\ne.g.: causal model-level prediction given …, associative causal data-level prediction given\nUse Cases and Application Example 1 Scientific model, trust in model, goal is knowledge generation; psychological dataset\n includes causes, effects, other variables  Example 2 Decision model, trust in prior knowledge, goal is debugging; medical diagnosis\nDiscussion consequences for development of iml: not suitable for many questions?\n why interpret the data through a model? when is refitting more important  consequences for interpretation: many methods are applied incorrectly?\nLimitations The taxonomy is not comprehensive. I.e. distinguish local/global, also does not include methods not fitting the focus (non tabular, model specific, surrogate models)\nRelated Work interpreting the model vs interpreting the data paper janzing feature relevance quantification\nConclusion bli bla blub\n  The learned model includes both the temperature in-store and the temperature back-room. Both variables are strongly correlated; in the model they cancel each other out, such that they do not contribute to the prediction on real-world data. However, for the PDP the correlation between the variables is broken, such that the variable has an effect and we cannot make conclusions about the correlations in the data. Furthermore, correlation is not causation. ↩︎\n Insight into the associations in the data is also not possible in this case. In fact there is only a very small association between in-store temperature and sales and it is negative (the PDP suggests the opposite). ↩︎\n Our notation is as follows: We denote the optimal prediction model as $f^*$, the estimated model as $\\hat{f}$. We suppose that the model was fitted on the covariates $X$ to predict $Y$. The model’s prediction is denoted as $\\hat{Y}$. Random variables are denoted with capital letters, the respective realizations as $X=x$ or just $x$. Conditional dependence is denoted as $X \\not \\perp Y|Z$ (and independence as $X \\perp Y |Z$). Probability distributions are denoted as $P(X)$, event probabilities as $p(x)$. To denote causal interventions we make use of the $do$-operator: to indicate that variable $X_j$ is set to value $\\theta_j$, we write $do(X_j=\\theta_j)$. Furthermore, we assume that the data generating process can be described with a structural causal model (SCM) where the values of the variables are determined by the mutually independent exogenous variables $U_j$, the structural equations $f_j$ and the observation of the causal parents $X_{pa(j)}$, i.e. that $X_j := f_j(X_{pa(j)}, U_j)$. The SCM induces a causal graph $\\mathcal{G}$, where each node is connected with its direct effects with a directed edge. We assume that $\\mathcal{G}$ is a directed acyclic graph (DAG). ↩︎\n   ",
  "wordCount" : "1711",
  "inLanguage": "en",
  "datePublished": "2022-04-16T00:30:03Z",
  "dateModified": "2022-04-16T00:30:03Z",
  "author":{
    "@type": "Person",
    "name": "Gunnar König"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://gunnarkoenig.com/posts/2022/interpretability-levels/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Gunnar König",
    "logo": {
      "@type": "ImageObject",
      "url": "https://gunnarkoenig.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://gunnarkoenig.com" accesskey="h" title="Gunnar König (Alt + H)">Gunnar König</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://gunnarkoenig.com/aboutme/" title="about">
                    <span>about</span>
                </a>
            </li>
            <li>
                <a href="https://gunnarkoenig.com/publications/" title="publications">
                    <span>publications</span>
                </a>
            </li>
            <li>
                <a href="https://gunnarkoenig.com/teaching/" title="teaching">
                    <span>teaching</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://gunnarkoenig.com">Home</a>&nbsp;»&nbsp;<a href="https://gunnarkoenig.com/posts/">Posts</a></div>
    <h1 class="post-title">
      If interpretability is the answer, what is the question?
    </h1>
    <div class="post-description">
      I propose a taxonomy of the types of questions that IML methods can answer.
    </div>
    <div class="post-meta"><span title='2022-04-16 00:30:03 +0000 +0000'>April 16, 2022</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Gunnar König

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#which-aspect-of-model-and-data-shall-be-described" aria-label="Which aspect of model and data shall be described?">Which aspect of model and data shall be described?</a><ul>
                        
                <li>
                    <a href="#which-object" aria-label="Which object?">Which object?</a></li>
                <li>
                    <a href="#on-what-level" aria-label="On what level?">On what level?</a></li></ul>
                </li>
                <li>
                    <a href="#classification-of-common-iml-techniques" aria-label="Classification of Common IML Techniques">Classification of Common IML Techniques</a></li>
                <li>
                    <a href="#relationships-between-categories" aria-label="Relationships between categories">Relationships between categories</a></li>
                <li>
                    <a href="#use-cases-and-application" aria-label="Use Cases and Application">Use Cases and Application</a><ul>
                        
                <li>
                    <a href="#example-1" aria-label="Example 1">Example 1</a></li>
                <li>
                    <a href="#example-2" aria-label="Example 2">Example 2</a></li></ul>
                </li>
                <li>
                    <a href="#discussion" aria-label="Discussion">Discussion</a></li>
                <li>
                    <a href="#limitations" aria-label="Limitations">Limitations</a></li>
                <li>
                    <a href="#related-work" aria-label="Related Work">Related Work</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<p>The goal of Interpretable Machine Learning (IML) is to provide human-intelligible descriptions of potentially complex ML models. However, IML descriptors themselves can be difficult to interpret: To reduce complexity, each method is limited to provide insight into <em>a specific aspect</em> of model and data. If the method&rsquo;s focus is not clearly understood, IML descriptors can be misinterpreted.</p>
<p>We illustrate the problem on a simple example: Suppose a practitioner aims to learn about the relevance of <em>in-store temperature</em> for <em>sales</em> at a petrol station. Therefore she fits a model on a sales dataset and computes the Partial Dependence Plot (PDP) for the temperature (Figure 1).</p>
<p><img loading="lazy" src="pdp-dummy.jpg" alt="Left: the data generating process, the corresponding causal graph. Right: The model&amp;rsquo;s mechanism and the partial dependence plot for temperature."  />
</p>
<p>Looking at the PDP output, she decides to turn up the theromstat in order to increase the sales. Her conclusion is not valid: In reality, increasing in-store temperature has a small negative causal effect on the sales.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> The reason is that the chosen method, PDP, is designed to provide insight into the model&rsquo;s mechanism, but in general does not allow insight into the effect of real-world actions.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>In this article, we aid practicitioners to chose suitable IML tools and to avoid misinterpretation of the IML descriptor. More specifically, we guide practitioners to</p>
<ol>
<li>carefully choose the aspect of model and data that shall be described</li>
<li>to understand which IML descriptors are suitable for their task.</li>
</ol>
<p>Therefore we present a taxonomy of the different aspects of model and data that may be of interest (<a href="/posts/2022/interpretability-levels/#taxonomy-aspects-of-model-and-data">here</a>) and classify existing established model-agnostic IML methods within that taxonomy (<a href="/posts/2022/interpretability-levels/#classification-of-common-iml-techniques">here</a>). <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<h1 id="which-aspect-of-model-and-data-shall-be-described">Which aspect of model and data shall be described?<a hidden class="anchor" aria-hidden="true" href="#which-aspect-of-model-and-data-shall-be-described">#</a></h1>
<p>As follows, we distinguish nine different aspects of model and data. Therefore, we distinguish IML descriptions along two questions:</p>
<ol>
<li>Which object is described? Is it the model&rsquo;s prediction $\hat{Y}$, the underlying real-world object $Y$ or their relationship?</li>
<li>On what level is the object described? I.e., are we interested in observing co-occurance or in understanding causal effects? And, if we are interested in causal effects: Are we interested in the effect of plugging in different values into the model, or into the effect of actions in the real-world?</li>
</ol>
<p>As follows, we demonstrate that the objects/levels of understanding are distinct and discuss when each of the categories may be of interest.</p>
<h2 id="which-object">Which object?<a hidden class="anchor" aria-hidden="true" href="#which-object">#</a></h2>
<p>Depending on the interpretation goal, we may either be interested in understanding</p>
<ul>
<li>the model&rsquo;s prediction $\hat{Y}$,</li>
<li>the prediction target $Y$,</li>
<li>the prediction loss $L$, or more generally the relationship between $\hat{Y}$ and $Y$.</li>
</ul>
<p>[TODO color code the bullets]</p>
<p><strong>Differentiation:</strong> First of all, $Y$ and $\hat{Y}$ are different objects: In general they take different values within the observational distribution (since in general $Y$ cannot be perfectly predicted from the covariates). Furthermore they take different causal roles and as such behave very differently in interventional environments. For example, changing the test result has a causal effect on the Covid prediction, but does not affect whether someone is infected with covid. Clearly, $Y$ and $\hat{Y}$ are different objects than e.g. their loss $L$. For instance, the loss $L$ takes different values than $Y$ or $\hat{Y}$ and also has a different causal role.</p>
<p><strong>Suitability:</strong> We have established that $Y$, $\hat{Y}$ and $R$ are different objects; the question which of the objects is of interest in which scenario remains. The practitioner has to carefully assess which of those is of interest in any situation; we cannot give general recommendations. However, we can roughly categorize as follows.
In scenarios where we trust the model and want to learn from it, e.g. in scientific inference, we typically aim to understand the underlying target $Y$. In these scenarios, the prediction is only a proxy for understanding $Y$.
In scenarios where we are sceptical of the model and the goal is to debug it, we typically aim to understand the prediction $\hat{Y}$. For instance, if the model relies on a protected attribute or a spuriously correlated variable, we would want to expose this, irrespective of whether that relationship is reflected in the underlying data generating process.
Understanding their relationship is for example interesting in situations where we want to understand the sensitivity of the model&rsquo;s performance to changes in the data generating process.</p>
<h2 id="on-what-level">On what level?<a hidden class="anchor" aria-hidden="true" href="#on-what-level">#</a></h2>
<p>We have established that there are three distinct objects which all may be of interest depending on the situation. In this Section we explain that we can understand each of the objects on different levels. More specifically, we may either be interested in understanding</p>
<ul>
<li>How is the object associated with the covariates (statistical dependence)? [What does that mean exactly]</li>
<li>How is the object affected by real-world action (i.e., interventions on light blue variables, $do(X_j=\theta_j)$)? [What does that mean why is it interesting]</li>
<li>How is the object affected by model-level interventions (i.e., interventions on purple variables, $do(\overline{X}_j=\theta_j)$)? ][What does that mean why is it interesting?]</li>
</ul>
<p>[TODO color code the bullets]</p>
<p><strong>Differentiation:</strong> For instance, suppose that we are interested in the model&rsquo;s prediction. Data-level causation is different from model-level causation: For instance intervening on trust on the model level does not affect the prediction, but intervening on the data-level affects the prediction via the vaccination status. Furthermore causation is different from association: <em>quarantine</em> is associated with the prediction, but neither model-level intervention nor data-level intervention on <em>quarantine</em> affect the prediction.</p>
<p><img loading="lazy" src="taxonomy-overview.jpg" alt="Image with a simple scenario which allows to distinguish all relevant aspects. Use the one from the IML lecture?"  />
</p>
<p><em>Subtitle: To illustrate our arguments, we make use of the example of a CoViD prediction model $\hat{f}$. The model is trained on variables such as the vaccination status or quick-test results and shall predict the risk of someone having covid. For the example we assume knowledge of the corresponding structural causal model (SCM), which we can use to sample data or to predict the effect of interventions in the real-world. The combination of model&rsquo;s mechanism and data generating process is illustrated below.</em></p>
<p><strong>Suitability:</strong> Describing objects with respect to the effects of data-level interventions is of interest if the description is meant to guide real-world interaction. For instance, recourse recommendations guide individuals towards reverting unfavorable decisions, or business analysists may be interested in understanding how to keep customers who are likely to churn. Estimating the effect of data-level interventions is in general difficult to obtain and not always necessary. For instance, to select the variables that are relevant to diagnose diseases understanding which variables are strongly associated with the disease is sufficient.
It has also been argued that, in order to understand the model&rsquo;s mechanism, we are interested in the effect of plugging in different values for each of the features to the model [cite janzing]. This allows for properties such as sensitivity, i.e. that nonzero effect implies nonzero coefficients.</p>
<p>It remains an open question when which of the levels is of interest. Typically either data-level causation or association. Data-level causation tells us how the the object of interest behaves if we act in the real-world. Association is helpful if we are only interested in co-occurance (which is e.g. required for prediction). The use of model-level causation is debated; some argue that this is exactly what we want if we are interested in in interpretability since we want to understand the model&rsquo;s mechanism, others argue that interpretation of the model in unrealsitic regions is not helpful at all and that model-level interventions lead to extrapolation into unrealistic environments.</p>
<h1 id="classification-of-common-iml-techniques">Classification of Common IML Techniques<a hidden class="anchor" aria-hidden="true" href="#classification-of-common-iml-techniques">#</a></h1>
<p>Overview table with the 9 categories.</p>
<table>
<thead>
<tr>
<th></th>
<th>prediction</th>
<th>target</th>
<th>performance</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>association</strong></td>
<td>M-Plot<!-- raw HTML omitted -->conditional SHAP</td>
<td>-</td>
<td>conditional SAGE<!-- raw HTML omitted -->CFI<!-- raw HTML omitted -->ASV</td>
</tr>
<tr>
<td><strong>causal (data-level)</strong></td>
<td>CR<!-- raw HTML omitted -->CSVs</td>
<td>MCR<!-- raw HTML omitted -->Hastie PDP</td>
<td>Invariant prediction importance</td>
</tr>
<tr>
<td><strong>causal (model-level)</strong></td>
<td>CXplain<!-- raw HTML omitted -->marginal SHAP</td>
<td>-</td>
<td>PFI<!-- raw HTML omitted -->marginal SAGE</td>
</tr>
</tbody>
</table>
<h1 id="relationships-between-categories">Relationships between categories<a hidden class="anchor" aria-hidden="true" href="#relationships-between-categories">#</a></h1>
<p>In principle mutually exclusive.
Demonstrate on examples in one figure (e.g. respective global relevance plot for each type in a 3x3 grid), proof in appendix.</p>
<p><strong>approach 1:</strong> assumptions and their consequences</p>
<ul>
<li>statistical independence of covariates</li>
<li>causal independence of covariates</li>
<li>observational identifiability of causal effect?</li>
<li>bayes optimal predictor</li>
<li>perfect predictor</li>
</ul>
<p><strong>approach 2:</strong> for each of the 9 classes, list which other classes imply the class and under which assumption they do so</p>
<p>e.g.: causal model-level prediction given &hellip;, associative causal data-level prediction given</p>
<h1 id="use-cases-and-application">Use Cases and Application<a hidden class="anchor" aria-hidden="true" href="#use-cases-and-application">#</a></h1>
<h2 id="example-1">Example 1<a hidden class="anchor" aria-hidden="true" href="#example-1">#</a></h2>
<p>Scientific model, trust in model, goal is knowledge generation; psychological dataset</p>
<ul>
<li>includes causes, effects, other variables</li>
</ul>
<h2 id="example-2">Example 2<a hidden class="anchor" aria-hidden="true" href="#example-2">#</a></h2>
<p>Decision model, trust in prior knowledge, goal is debugging; medical diagnosis</p>
<h1 id="discussion">Discussion<a hidden class="anchor" aria-hidden="true" href="#discussion">#</a></h1>
<p>consequences for development of iml: not suitable for many questions?</p>
<ul>
<li>why interpret the data through a model?</li>
<li>when is refitting more important</li>
</ul>
<p>consequences for interpretation: many methods are applied incorrectly?</p>
<h1 id="limitations">Limitations<a hidden class="anchor" aria-hidden="true" href="#limitations">#</a></h1>
<p>The taxonomy is not comprehensive. I.e. distinguish local/global, also does not include methods not fitting the focus (non tabular, model specific, surrogate models)</p>
<h1 id="related-work">Related Work<a hidden class="anchor" aria-hidden="true" href="#related-work">#</a></h1>
<p>interpreting the model vs interpreting the data paper
janzing feature relevance quantification</p>
<h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>bli bla blub</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>The learned model includes both the temperature in-store and the temperature back-room. Both variables are strongly correlated; in the model they cancel each other out, such that they do not contribute to the prediction on real-world data. However, for the PDP the correlation between the variables is broken, such that the variable has an effect and we cannot make conclusions about the correlations in the data. Furthermore, correlation is not causation.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Insight into the associations in the data is also not possible in this case. In fact there is only a very small association between in-store temperature and sales and it is negative (the PDP suggests the opposite).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Our notation is as follows: We denote the optimal prediction model as $f^*$, the estimated model as $\hat{f}$. We suppose that the model was fitted on the covariates $X$ to predict $Y$. The model&rsquo;s prediction is denoted as $\hat{Y}$. Random variables are denoted with capital letters, the respective realizations as $X=x$ or just $x$.  Conditional dependence is denoted as $X \not \perp Y|Z$ (and independence as $X \perp Y |Z$). Probability distributions are denoted as $P(X)$, event probabilities as $p(x)$.  To denote causal interventions we make use of the $do$-operator: to indicate that variable $X_j$ is set to value $\theta_j$, we write $do(X_j=\theta_j)$. Furthermore, we assume that the data generating process can be described with a structural causal model (SCM) where the values of the variables are determined by the mutually independent exogenous variables $U_j$, the structural equations $f_j$ and the observation of the causal parents $X_{pa(j)}$, i.e. that  $X_j := f_j(X_{pa(j)}, U_j)$. The SCM induces a causal graph $\mathcal{G}$, where each node is connected with its direct effects with a directed edge. We assume that $\mathcal{G}$ is a directed acyclic graph (DAG).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://gunnarkoenig.com/tags/interpretability/">interpretability</a></li>
      <li><a href="https://gunnarkoenig.com/tags/taxonomy/">taxonomy</a></li>
    </ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://gunnarkoenig.com">Gunnar König</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
